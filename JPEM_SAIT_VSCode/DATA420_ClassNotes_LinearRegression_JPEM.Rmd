---
title: "Introduction to R Part III: Supervised Models - Regression"
author: "Jason Pemberton"
date: "2023-09-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Libraries, include=FALSE}
library(tidyverse)
library(readr)
library(knitr)
library(dplyr)
library(gmodels)
library(readxl)
library(ggpmisc)
library(scales)
library(car)
library(plotly)
library(class)
library(caret)
library(gridExtra)
```

Predictive models are given instructions on what they need to learn and how they are intended to learn it, the process of training a predictive model is known as **supervised learning**. Supervision does not mean "human involvement", but rather the fact that the target values provide a way for the learner to know how well it has learned the task. Given a set of data, a supervised learning algorithm attempts to optimize a function (the model) to find the best combination of feature values, resulting in a target output across all rows in the training data.

To begin applying machine learning to a real-world project, you will need to determine which of the four learning tasks your project represents: classification (logistic regression, KNN), numeric prediction (linear regression & trees), pattern detection (association), or clustering (K-Means).

------------------------------------------------------------------------

### [**STATISTICS - LINEAR & LOGISTIC REGRESSION**]{.underline}

Linear Regression is used to predict a continuous outcome while Logistic Regression is used to predict a categorical outcome

Examples:

Recall in DATA 415 we worked on a Linear Regression problem predicting house prices (continuous) using independent variables such as square footage, \# of bedrooms, \# of floors etc. Another example might use a persons weight or height (continuous) to predict their blood pressure (continuous).

Logistic Regression can be used to calculate the probability of an event such as predicting whether a tumour is benign or malignant (categorical) or predicting a credit score rating. A Logistic Regression outcome will lie between 0 and 1 (a probability of occurring).

------------------------------------------------------------------------

#### **LINEAR REGRESSION**

The basic structure of a linear regression in R is:

lm([target] \~ [predictor / features], data = [data source])

or

model \<- lm(Y\~X, data)

Y = target or dependent variable

X = predictor or independent variable

------------------------------------------------------------------------

#### **PENGUINS DATA ANALYSIS USING LINEAR REGRESSION**

```{r message=FALSE, warning=FALSE}
# Load the penguins database
penguins <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/COURSE - NEW_SAIT_DATA420/data/penguins.csv")

# Create a linear model using the lm function
model <- lm(bill_length_mm~flipper_length_mm, data=penguins)

# display the model results
summary(model)

# Create a scatterplot of the two variables 
penguins %>% 
  ggplot(aes(flipper_length_mm, bill_length_mm))+
  geom_point()+
  geom_smooth(method = lm, se=FALSE)+ #lm=linear model se=FALSE removes error bars
  stat_poly_eq()+ #add R-Squared using the ggpmisc package
  labs(title="Penguins Data Analysis - Linear Regression",
       x="Flipper Length (mm)",
       y="Bill Length (mm)")

penguins %>% 
  ggplot(aes(flipper_length_mm))+
  geom_histogram()

penguins %>% 
  ggplot(aes(bill_length_mm))+
  geom_histogram()

model_glm <- glm(bill_length_mm~flipper_length_mm, data=penguins, family = "poisson")
summary(model_glm)
```

The ***lm*** function returns residuals, R-Squared, and the coefficients necessary to create your linear model (y=mx+b where b=intercept, and m=Coefficient(flipper_length_mm))

You can now use this equation (your model) to make future predictions on penguin bill length (target) using flipper length (predictor).

Based on the models results, R-squared=0.43. This means Flipper Length can explain 43% of the variation we see in Bill Length. Note: The dependant (target) variable is the second item in the GGPLOT aes function (y-axis)

------------------------------------------------------------------------

#### **HOUSE PRICE PREDICTION USING LINEAR REGRESSION**

```{r}
HousePrices <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/COURSE - NEW_SAIT_DATA420/data/HousePrices.csv")

head(HousePrices, 5)

# Linear Regression predicting price (dependent) vs area (independent)
model1 <- lm(price~area, data=HousePrices)
summary(model1) 

p <- HousePrices %>% 
  ggplot(aes(x=area, y=price))+
  geom_point()+
  #geom_smooth(method = lm, se=FALSE)+ #lm=linear model se=error bars
  stat_poly_line(se=FALSE)+
  stat_poly_eq(use_label(c("eq", "R2", "p")))+ #add R-Squared, p and equation using the ggpmisc package
  labs(title="House Price Data Analysis - Linear Regression",
       x="Area (sq ft)",
       y="Price ($)")+
  theme_minimal()+
  scale_y_continuous(
    breaks=seq(0, 1500000, 100000),
    labels = label_dollar()) # using scales library

ggplotly(p)

```

For this single variable linear regression we achieved an R-Squared value of 0.29 (or 29%). We interpret this as follows: 29% of the variation in house price can be explained by its area.

------------------------------------------------------------------------

#### **HOUSE PRICE PREDICTION USING MULTIPLE LINEAR REGRESSION**

Recall in the DATA 415 practice problem we created a multiple linear regression predicting house prices based on a number of variables. We can add additional independent variables to our model by inserting the "+" symbol between each variable.

In this case, we need to look at the Adjusted R-Squared value to see how our model performs.

We cannot create a 2D scatterplot of house prices versus multiple variables but we can use the ***avPlots*** function available in the ***car*** package to quickly create each of the individual linear regression plots.

```{r Multiple Linear Regression}
 
model2 <- lm(price~area+bedrooms+bathrooms+stories+parking+age, data=HousePrices)
summary(model2) 

# package: car (Companion to Applied Regression)
# avPlots (added variable plots)
avPlots(model2)
```

The adjusted R-Squared for this model is 56%. Individually none of the independent variables achieved an R-Squared higher than 29%. So our multiple variable analysis has produced a better model to predict house price.

------------------------------------------------------------------------

### **ACCESSING MODEL DATA AND USING MODELS TO PREDICT NEW DATA**

By assigning the output of the LM function to a variable we can explore the model results using functions such as: ***summary***, ***coef***, ***anova*** and we can use our model on new data with the ***predict*** function

Analysis of Variance: ANOVA (or AOV) is short for ANalysis Of VAriance. ANOVA is one of the most basic yet powerful statistical models you have at your disopsal. While it is commonly used for categorical data, because ANOVA is a type of linear model it can be modified to include continuous data.

We can also use the ***predict*** function to apply our model to new data

```{r}

coef(model1)
#residuals(model1)
anova(model1)

newdata <- data.frame(
  area = c(2010, 3002, 1001, 4050, 5002, 6004)
)

predict(model1, newdata)
```

```{r}

model3 <- lm(price~area+age, data=HousePrices)

summary(model3)

coef(model3)
#residuals(model3)
anova(model3)

newdata2 <- data.frame(
  area = c(2010, 3002, 1001, 4050, 5002, 6004),
  age = c(21, 55, 3, 12, 45, 16)
)

predict(model3, newdata2)

```

An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable

model \<- lm(Y\~X1\*X2, data)

```{r}

model4 <- lm(price~area*age, data=HousePrices)

summary(model4)

coef(model4)
#residuals(model4)
anova(model4)

predict(model4, newdata2)
```

A linear transformation preserves linear relationships between variables. There are many ways to transform variables to achieve linearity for regression analysis. Some common methods are ***log*** and ***poly***(nomial)

```{r}
model5 <- lm(log(price)~area, data=HousePrices)

summary(model5)
coef(model5)
anova(model5)

predict(model5, newdata2)

model6 <- lm(price~poly(area, 2), data=HousePrices)
summary(model6)
coef(model6)
anova(model6)

predict(model6, newdata2)
```

Linear Regression: X is Numeric

Linear models assume:

-   Normal distribution of underlying data

-   Data points are independent

for non-normal distributions we use GLM, Generalized Linear Model

```{r}
model7 <- glm(price~area, data=HousePrices, family="poisson")
summary(model7)
anova(model7)

HousePrices %>% 
  ggplot(aes(area))+
  geom_histogram()+
  labs(title="House Area Distribution",
       x="Area")

HousePrices %>% 
  ggplot(aes(price))+
  geom_histogram()+
  labs(title="House Price Distribution",
       x="Price")
```

HousePrice data appears to be skewed to the left, rather than normally distributed - so a GLM might be preferable
