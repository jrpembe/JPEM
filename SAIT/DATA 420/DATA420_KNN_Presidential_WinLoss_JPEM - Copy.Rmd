---
title: "KNN - Presidential Win/Loss"
author: "Jason Pemberton"
date: "2023-11-06"
output:
  html_document:
    theme: united
    highlight: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Libraries, include=FALSE}
# Load the required libraries
library(tidyverse)
library(caret)
library(e1071) # for KNN
library(readr)
library(plotly)
```

KNN is a supervised learning technique in which we try to classify data points to a given category with the help of training set. KNN captures information from all training data and classifies new data based on similarity. KNN is a supervised learning algorithm where the target (dependent variable) is known

Recall in DATA 415 we performed KNN using a data set of football player's height & weight and their uniform sizes. When provided with new player information, we calculated Euclidean distance between the new player data and the existing data to determine the correct uniform size of the new player. Once we calculated the distances, we sorted them from lowest to highest (nearest to farthest) and assigned a uniform size based on the K nearest neighbours.

In this example of KNN using R we are going to look at historical data of past win/loss statistics for US presidents and their corresponding speeches. This dataset comprises of 1524 observations on 14 variables. The dependent variable is win/loss where 1 indicates win and 0 indicates loss. The independent variables are:

1.  Proportion of words in the speech showing

    a.  Optimism

    b.  Pessimism

    c.  The use of Past

    d.  The use of Present

    e.  The use of Future

2.  Number of times he/she mentions his/her own party

3.  Number of times he/she mentions his/her opposite parties

4.  Some measure indicating the content of speech showing

    a.  Openness

    b.  Conscientiousness

    c.  Extraversion

    d.  Agreeableness

    e.  Neuroticism

    f.  Emotionality

These variables have a wide range of numeric values and so we must perform scaling on the data to prevent one variable from dominating the model. Also our target variable (Win/Loss) must be converted to a factor in order for KNN to work.

```{r Read data, include=FALSE}
# Read the CSV file
data <- read_csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/COURSE - NEW_SAIT_DATA420/data/US Presidential Data.csv")
```

```{r Data Preparation}
# Turn your target (dependent) variable into a factor
data$`Win/Loss` <- as.factor((data$`Win/Loss`))

# Set the seed for reproducibility
set.seed(123)

# Split the data into independent features (X) and the target variable (Y)
X <- data[, -1]  # Exclude the "Win/Loss" column
Y <- data$`Win/Loss`


# Standardize the features - large differences in data range between variables 
# can lead to one variable dominating your model
X_scaled <- scale(X)
```

```{r Data Partition}
# Split the data into training and testing sets (80% training, 20% testing)
splitIndex <- createDataPartition(Y, p = 0.8, list = FALSE)
X_train <- X_scaled[splitIndex, ]
X_test <- X_scaled[-splitIndex, ]
Y_train <- Y[splitIndex]
Y_test <- Y[-splitIndex]
```

```{r Training Model}
# Train the KNN model
# You can choose the value of k. The square root of the number of rows is one option. 
# For this example we will run a for loop to test a range of K values from 1 to 39

results <- list()
x_data <- (1:39)

for (i in 1:39) {
  
  model <- train(
    x = X_train,
    y = Y_train,
    method = "knn",
    trControl = trainControl(method = "cv"),
    tuneGrid = expand.grid(k = i)
  )
  results <- append(results, model$results$Accuracy)
}

# find the max value of accuracy in the KNN model
optimal_k <- which.max(results)

plot(unlist(results))
points(optimal_k, results[optimal_k], pch = 19, col = "red")

# Add a label to the highlighted point
text(optimal_k, results[optimal_k], labels = paste("Optimal k =", optimal_k), pos = 1, col = "red")

```

```{r}
# Make predictions
predictions <- predict(model, newdata = X_test)

# Evaluate the model
confusionMatrix(predictions, Y_test)

```

Looking at the results of our KNN model in a confusion matrix allows us to visualize the model's accuracy:

![](images/Screenshot%202023-11-06%20135305.jpg)
