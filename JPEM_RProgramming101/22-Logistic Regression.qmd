---
title: "Logistic Regression"
author: "Jason Pemberton"
format: html
code-fold: true
editor: visual
toc: true
toc-location: left
code-annotations: hover
theme: cyborg
---

```{r}
#| label: Load Libraries
#| message: false
#| warning: false
#| include: false
library(tidyverse)
library(plotly)
library(gtExtras)
library(mlbench)
library(reshape2)
library(RColorBrewer)
library(broom)
```

## What is Logistic Regression?

::: callout.red-bg
Logistic regression is a statistical method used to model and analyze datasets where the outcome variable is binary (e.g. "yes/no", or "male/female") and predictor variables are those believed to either influence, predict, or explain the outcome.
:::

The best way to learn logistic regression is to look at an example (and try to replicate the analysis yourself). Use R to analyze a dataset called PimaIndiansDiabetes (accessed by installing mlbench).

The dataset has:

**A binary outcome variable:**

-   diabetes (positive or negative)

**Predictor variables:**

-   pregnant, number of times pregnant

-   glucose, blood glucose levels

-   pressure, diastolic blood pressure

-   triceps, triceps skin fold thickness

-   insulin, blood insulin levels

-   mass, body mass index (BMI)

-   pedigree, diabetes pedigree function

-   age, age in years

::: callout-note
Objective: To determine which predictor variables are associated with meaningful changes in the probability of being diagnosed with diabetes and to quantify the extent of their influences.
:::

Understand how predictors influence the probability of diabetes can inform risk stratification, policy, or clinical interventions.

Here is a snapshot of the first ten rows of data:

```{r}
data("PimaIndiansDiabetes") #<1>

# Prepare and display the table
PimaIndiansDiabetes %>%
  select(pregnant, glucose, pressure, triceps, insulin, mass, pedigree, age, diabetes) %>%
  mutate(diabetes = ifelse(diabetes == "pos", "Positive", "Negative")) %>%
  head(10) %>%  # showing just a sample for clarity
  gt() %>%
  tab_header(title = "PimaIndiansDiabetes Dataset") %>%
  tab_style(
    style = list(cell_fill(color = "lightgreen")),
    locations = cells_body(
      columns = diabetes,
      rows = diabetes == "Positive"
    )
  ) %>%
  tab_style(
    style = list(cell_fill(color = "mistyrose")),
    locations = cells_body( #<2>

      columns = diabetes,
      rows = diabetes == "Negative"
    )
  )
```

1.  This is a comment

2.  This is another comment

## A Single Numeric Predictor

Let's start with a single numeric predictor. In this case "glucose" and take look at the relationship between changing levels of glucose and the likelihood of being diagnosed with diabetes. @fig-scatter-plot shows the diagnostic status for each value in the dataset of glucose. It is clear that for lower levels of glucose, fewer people are diagnosed with diabetes and for higher levels of glucose, more people are diagnosed with diabetes.

Logistic regression uses that data to create a model than can be used to predict the probability of being diagnosed with diabetes at any level of glucose. @fig-logistic-regression shows how that probability changes over the values of glucose levels.

::: panel-tabset
## Distribution

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-scatter-plot
#| fig-cap: "Glucose Levels and Diabetes Status"
PimaIndiansDiabetes %>%
  ggplot(aes(x = glucose, y = as.numeric(diabetes == "pos") * 0.5, color = diabetes)) +
  geom_jitter(height = 0.2, width = 0.2, alpha = 0.3, size = 4) +
  scale_color_manual(values = c("neg" = "darkgreen", "pos" = "red")) +
  labs(
    title = "Glucose Levels and Diabetes Status",
    x = "Glucose Levels"
  ) +
  scale_y_continuous(
    breaks = c(0, 0.5),
    labels = c("No Diabetes", "Diabetes")
  ) +
  theme_minimal() +
  theme(
    legend.position = "none",
    axis.line.x = element_line(colour = "black"),
    plot.title = element_text(hjust = 0.5),
    axis.title.y = element_blank()
  )
```

## Probability

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-logistic-regression
#| fig-cap: "Predicted Probability of Diabetes by Glucose Levels"
model <- glm(diabetes ~ glucose, data = PimaIndiansDiabetes, family = "binomial")

# Generate data for the curve
glucose_range <- data.frame(glucose = seq(min(PimaIndiansDiabetes$glucose),
                                          max(PimaIndiansDiabetes$glucose),
                                          length.out = 500))

glucose_range$prob <- predict(model, newdata = glucose_range, type = "response")

# Plot the predicted probability
plot_ly(data = glucose_range,
        x = ~glucose,
        y = ~prob,
        type = 'scatter',
        mode = 'lines',
        name = "Probability Curve") %>% 
  layout(
    title = "Probability of Diabetes by Glucose Level",
    xaxis = list(title = "Glucose Level"),
    yaxis = list(title = "Probability of Diabetes")
  )
```
:::

Interpreting the model results:

Rather than a simple linear regression with equation y=mx+b, we are dealing with probability of an outcome, so our equation is:

log(P / (1 - P)) = -5.350080 + 0.037873 × glucose

Solving for Probability (p):

**P = 1 / (1 + exp( - ( -5.350080 + 0.037873 × glucose ) ))**

## Creating a simple model

Here is the code to create and summarize a logistic regression model with this data (followed by results).

```{r}
#| message: false
#| warning: false
model2 <- glm(diabetes ~ glucose,
              data = PimaIndiansDiabetes,
              family = binomial)

summary(model2)
```

### Explanatory Notes

-   glm() is used to fit a generalized linear model

-   The formula diabetes \~ glucose specifies that we are modelling diabetes as the outcome, predicted by glucose

-   The family = binomial argument indicates that we are using logistic regression (appropriate for binary outcomes

-   summary(model) provides detailed results of the model

Intercept tells us what the model looks like when glucose is zero (where the model intercepts the y-axis). This is meaningless in this example because people can't live with zero glucose. So we can ignore that number.

We have coefficients for glucose, and our predictor variable. This will tell us what happens when the value of glucose changes. Specifically how it affects a person being diagnosed with diabetes. The change can be positive, negative, or zero. A positive number means that as glucose levels go up, the chances of being diagnosed with diabetes goes up too. If it were zero, there would be no change. If it were negative, it would mean that as glucose goes up, the chances of being diagnosed with diabetes would go down.

We use the term "chances" and not "probability" because the actual number is what is referred to as "long-odds" or the logarithm of the odds ratio.

## A Single Categorical Predictor

Now let's consider the same question using a categorical variable as predictor. The dataset doesn't have a categorical variable using the age variable - dividing observations into "Young adult", "Middle aged", "Older adult ". We're doing this to illustrate how to interpret the logistic regression results. In reality, it would be best practice to leave the Age variable as numeric and include it in the model.

Here is the code to create a categorical variable from a numeric variable:

```{r}
#| message: false
#| warning: false
#| label: fig-age-proportion
#| fig-cap: "Diabetes Proportion by Age Group"
# Prepare the data
age_data <- PimaIndiansDiabetes %>%
  mutate(age_group = case_when(
    age < 30 ~ "Young adult",
    age <= 50 ~ "Middle aged",
    TRUE ~ "Older adult"
  )) %>%
  mutate(age_group = factor(age_group, levels = c("Young adult", "Middle aged", "Older adult"))) %>% 
  count(age_group, diabetes) %>%
  group_by(age_group) %>%
  mutate(proportion = n / sum(n) * 100)

# Plot the chart
ggplot(age_data, aes(x = age_group, y = proportion, fill = diabetes)) +
  geom_col(position = "stack", width = 0.7) +
  scale_fill_manual(
    values = c("neg" = "lightgreen", "pos" = "lightcoral"),
    labels = c("neg" = "No diabetes", "pos" = "Diabetes")
  ) +
  labs(
    x = "Age Group",
    y = "Proportion (%)",
    fill = "Diabetes Status",
    title = "Diabetes Proportions by Age Group"
  ) +
  theme_minimal()
```

@fig-age-proportion shows the proportion of observations that have a diagnosis of diabetes changes across these three categories.

Now let's create a logistic regression model using age group (a categorical variable) as the predictor

```{r}

age_data2 <- PimaIndiansDiabetes %>%
  mutate(age_group = case_when(
    age < 30 ~ "Young adult",
    age <= 50 ~ "Middle aged",
    TRUE ~ "Older adult"
  )) %>%
  mutate(age_group = factor(age_group, levels = c("Young adult", "Middle aged", "Older adult")))

model3 <- glm(diabetes ~ age_group,
              data = age_data2,
              family = binomial)

summary(model3)
```

Under the heading "Coefficients" you'll notice there is no row for "Young adult". This is because it is our reference category. This is how R handles **categorical variables** in regression models. When you include a factor like `age_group`, R uses what's called **dummy coding** (or *treatment coding*). It picks one of the factor levels to be the **reference category**, and all the other levels are compared to it. “**Young adult**” is the **baseline** or reference group. The regression intercept represents the log-odds of diabetes for this group, and the coefficients for “Middle aged” and “Older adult” tell you how much *more* (or less) likely those groups are to have diabetes compared to “Young adult.”

## Adding Variables

By considering multiple predictors simultaneously, logistic regression allows us to exploit how each variable contributes to the likelihood of the outcome (in this case, diabetes) while accounting for the influence of others.

In the case of risk of diabetes, there may be multiple risk factors that we want to consider. Let's start by adding variables to our model and see what happens. We'll start by adding age to the model (the original age, numeric variable, not the categorical variable we created.

One of the key strengths of logistic regression is the ability to include multiple predictor variables in the model. For example, beyond plasma glucose concentration, we can include variables such as age, weight, pregnancy history, and blood pressure for example.

::: panel-tabset
## Two predictors

```{r}
model4 <- glm(diabetes ~ glucose + age,
              data = PimaIndiansDiabetes,
              family = binomial)

summary(model4)
```

**Glucose**:

Looking at the results, we can see that glucose remains a positive and statistically significant predictor of diabetes. The coefficient is slightly lower than in the model without age. This reflects the adjusted effect of glucose on the likelihood of being diagnosed with diabetes, accounting for the effect of age. In other words, some of the previously observed effect of glucose on diabetes risk can now be attributed to the different ages of the subjects rather than solely on their glucose levels.

**Age**:

Age also has a positive effect on the likelihood of being diagnosed with diabetes, and the small p-value confirms this effect is statistically significant. Because glucose is included in the model, the age effect is adjusted for glucose.

Explanatory notes on other results:

1.  **Akaike Information Criteria (AIC)**:
    -   A metric for comparing models, balancing goodness of fit with model complexity
    -   Lower AIC values suggest better trade-off between fit and complexity. Adding variables that improve the model without overfitting typical results in a lower AIC.
    -   The AIC (803.36) is also lower than the glucose-only model (814.59) indicating that adding age improves the model's overall fit.
2.  **Residual Deviance**:
    -   Measures how well the model fits the data. It is a measure of the discrepancies between observed outcomes and the model's predictions.
    -   Lower values indicate better fit. A drop in residual deviance when adding a variable suggests the new variable improves the model.
    -   The residual deviance (797.36) is lower than that of the glucose-only model (808.59), showing improved fit with the addition of age.
3.  **Null Deviance**:
    -   Represents the deviance of a model with no predictors (only an intercept)
    -   This is the baseline for comparison, showing how much better the model predictors perform compared to a model with no predictors.
    -   When to use: Null deviance is useful in understanding the baseline dataset. For instance, if the null deviance is very high and the residual deviance of your model remains close to it, it indicates that your predictors are not significantly improving the model's fit, and the model may not be capturing meaningful relationships in the data.
4.  **Number of Fisher Score Iterations:**
    -   Refers to the number of iterations the algorithm needed to converge on a solution when estimating model parameters.
    -   Fewer iterations indicates faster convergence, but this metric is not typically used to evaluate model quality.
    -   When to use: If a model takes an unusually high number of iterations to converge, it might signal an issue with the data or the model, such as multicollinearity or poorly scaled variables. Hypothetical example: If a logistic regression model takes 50 iterations (instead of 4-5) to converge, you might check for these issues or simplify the model.
:::

::: {.callout-important appearance="simple"}
We now have the tools we need to build a model. We can add numeric and categorical variables and look at the p-value of the coefficients to determine if its predictive value is statistically significant and then look at the residual deviance and AIC, which if lower, indicate the model is improved by adding that variable.
:::

Before building the model however, there are a few concepts and ideas that we need to explore that will help us understand how variables interact with each other.

## Confounding Variables

Confounding occurs when a third variable influences both the predictor and the outcome, distorting the relationship between the predictor and the outcome.

::: {.callout-important appearance="simple"}
Confounding creates a spurious association that might disappear or change when the confounding variable is controlled for by including it in the model.
:::

In this mode, for example, we should ask if being older (age) has an effect on the likelihood of diabetes (the outcome variable) AND glucose levels. If that were true, then some of the effect of glucose on diabetes could be accounted for simply by virtue of the fact that older people have higher glucose levels AND older people are more likely to have diabetes.

@fig-smooth-plot shows the relationship between age and glucose levels in the dataset:

```{r}
#| message: false
#| warning: false
#| label: fig-smooth-plot
#| fig-cap: "Glucose Levels vs Age"
PimaIndiansDiabetes %>% 
  ggplot(aes(age, glucose)) +
  geom_smooth(method = lm, se = F) +
  labs(
    title = "Glucose Levels vs Age",
    x = "Age",
    y = "Glucose Level"
  ) +
  theme_minimal()
```

## Effect Modifiers

Effect modifiers (or interacting variables) are slightly different. Let's stick with the example of glucose and age. While it is true that older people have higher glucose, it might also be true that being older (due to changes in physiology with age) modifies the effect of glucose on diabetes risk. So not only do older people have higher levels of glucose, but the glucose itself has a different effect on diabetes in older people.

Let's repeat the model but this time, instead of just adding age as an additional predictor variable, let's add it as a possible interaction term (or effect modifier) and see what happens.

```{r}
effect_model <- glm(diabetes ~ glucose * age, family = binomial, data = PimaIndiansDiabetes)

summary(effect_model)
```

First we notice that the interaction term is statistically significant and that the residual deviance and AIC are both reduced. All of this suggests that this interactions has improved the model and that the interaction term should be included in the model.

Next we see that the interaction term has a negative value for its estimate. Meaning that with increasing age, the effect of glucose on diabetes decreases. In other words, high levels of glucose impacts young people more than older people with respect to the likelihood of a diabetes diagnosis.

## Collinearity

Collinearity in logistic regression happens when two or more predictor variables are highly correlated with each other. It can lead to unstable estimates, inflated standard errors, and difficulty in interpreting the unique contribution of each predictor.

::: callout-important
Collinearity makes it difficult for the models to figure out the individual impact of each variable on the outcome because they are essentially explaining the same variance.
:::

It is a good idea to create a table or use data visualization to identify pairs of variables that are highly correlated. A correlation coefficient of 0.7 or higher may mean that you need to consider the strategies described below. from @fig-corr-matrix and @fig-correlation it is clear that none of the variables are highly correlated and so further action is not required.

::: panel-tabset
### Values

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-corr-matrix
#| fig-cap: "Correlation Matrix"
correlation_matrix <- cor(PimaIndiansDiabetes[ ,sapply(PimaIndiansDiabetes, is.numeric)])

cor_df <- as.data.frame(round(correlation_matrix, 2)) %>% 
  tibble::rownames_to_column("Variable")

cor_df %>% 
  gt() %>% 
  gt_color_rows(
    columns = -Variable,
    palette = "RdBu",
    domain = c(-1,1)
  )
```

### Scale

```{r}
#| message: false
#| warning: false
#| label: fig-correlation
#| fig-cap: "Correlation Plot"
# Compute correlation matrix
corr <- cor(PimaIndiansDiabetes[, sapply(PimaIndiansDiabetes, is.numeric)])

# Melt the matrix into long format
corr_melted <- melt(round(corr, 2))

# Plot heatmap
ggplot(corr_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "red", high = "blue", mid = "white",
    midpoint = 0, limit = c(-1, 1),
    name = "Correlation"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = "", y = "") +
  coord_fixed()
```
:::

Strategies for dealing with collinearity: Remove one of the Collinear Variables:\
If two variables are highly correlated, consider dropping one from the model. Especially if it provides redundant information.

## Building the Model

```{r}
model_glucose <- glm(formula = diabetes ~ glucose + age + glucose:age + pregnant + age:pregnant + mass + pressure + pedigree, family = binomial, data = PimaIndiansDiabetes)

summary(model_glucose)
```

## Checking Model Assumptions

Model assumptions can be checked visually and using statistical assumptions. these methods should be used together to get a sense of the strength of your model. We'll make reference to these plots in the text below. @fig-log-odds shows the log odds of diabetes for each predictor.

### Visual Checks

::: panel-tabset
## Linearity

```{r}
#| echo: true
#| message: false
#| warning: false
#| label: fig-log-odds
#| fig-cap: "Log Odds"


# Load the data and apply filters
dataset <- PimaIndiansDiabetes %>%
  filter(mass > 10, pressure > 10, age < 70) %>%
  na.omit()  # remove rows with missing values

# Fit the model on the filtered dataset
model <- glm(diabetes ~ pregnant + pressure + mass + pedigree + age,
             family = binomial, data = dataset)

# Generate predictions on the SAME dataset
log_odds <- predict(model, newdata = dataset, type = "link")

# Now the dimensions will match
dataset$log_odds <- log_odds


long_dataset <- dataset %>% 
  select(-diabetes, -insulin, -triceps) %>% 
  pivot_longer(cols = -log_odds, names_to = "predictor", values_to = )

long_dataset %>% 
  ggplot(aes(x = value, y = log_odds)) +
  geom_point(alpha = 0.2, colour = "steelblue") +
  geom_smooth(method = "loess", colour = "coral") +
  facet_wrap(~ predictor, scales = "free_x") +
  labs(
    title ="Log-Odds vs Predictors",
    x = "",
    y = "Log-Odds"
  ) +
  theme_bw()
```

## Cook's Distance

Cook's Distance is a measure used to identify influential data points in a regression model. It quantifies the impact of removing an observation on the overall model fit. High Cook's Distance values indicate that the observation has a significant influence on the estimated regression coefficients. A rule of thumb is that Cook's Distance greater than 1 or significantly higher than the values of most of the observations warrants further investigation. However, interpretation depends on context and the dataset. Our goal is to look for Cook's distances that are significantly less than 1. We can see in @fig-cooks-distance that none of the distances are more than 1.

```{r}
#| label: fig-cooks-distance
#| fig-cap: "Cook's Distance"
# Calculate Cook's distance
cooks_d <- cooks.distance(model)

# Create a data frame with index and Cook's D
cooks_df <- data.frame(index = 1:length(cooks_d),
                       cooks_distance = cooks_d)


ggplot(cooks_df, aes(x = index, y = cooks_distance)) +
  geom_segment(aes(x = index, xend = index, y = 0, yend = cooks_distance),
               color = "darkblue", alpha = 0.5) +
  geom_point(color = "coral", alpha = 0.6) +
  geom_hline(yintercept = 4 / nrow(cooks_df), linetype = "dashed", color = "gray50") +
  labs(title = "Cook’s Distance vs Observation Index",
       x = "Observation Index",
       y = "Cook’s Distance") +
  theme_minimal()

```

## Leverage Values

Leverage values measure the influence of an observation on the predicted outcomes by evaluating how far the observation's predictor values deviate from the mean of the predictors. In the context of logistic regression, observations with high leverage have the potential to dis proportionally affect a model's fit. High leverage points may not always be problematic, but they warrant attention., particularly if they have large residuals or Cook's Distances. Identifying and assessing leverage values ensures that influential points are not unduly skewing the model, which is critical for the robustness and intractability of the results.

Unlike Cook's Distance, there are some real outliers in the Leverage Values calculation. @fig-leverage-values shows some outliers that we will need to investigate.

```{r}
#| label: fig-leverage-values
#| fig-cap: "Leverage Values"
# Calculate leverage (hat) values
leverage_vals <- hatvalues(model)

# Create data frame for plotting
leverage_df <- data.frame(index = 1:length(leverage_vals),
                          leverage = leverage_vals)

ggplot(leverage_df, aes(x = index, y = leverage)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_hline(yintercept = 2 * (length(coef(model)) / nrow(leverage_df)),
             linetype = "dashed", color = "gray50") +
  labs(title = "Leverage (Hat) Values vs Observation Index",
       x = "Observation Index",
       y = "Leverage") +
  theme_minimal()
```

Identify specific observations with high leverage values:

```{r}
# Calculate leverage values
leverage_values <- hatvalues(model)

# Create dataframe with leverage values and observation index
leverage_df <- data.frame(
  Observation = seq_along(leverage_values),
  Leverage = leverage_values
)

# Define leverage threshold
n <- nrow(PimaIndiansDiabetes)
p <- length(coefficients(model))
leverage_threshold <- 2 * p / n

# Identify high-leverage observations
high_leverage_df <- leverage_df %>%
  filter(Leverage > leverage_threshold)

# Display top 5 by leverage
high_leverage_df %>%
  arrange(desc(Leverage)) %>%
  head(5)
```

## Deviance Residuals

Deviance Residuals measure the discrepency between observed and predicted values in logistic regression, providing a way to assess individual data point's fit within the model. They are derived from the contribution of each observation to the overall deviance, with large deviance residuals indicating observations that deviate significantly from the model's predictions. This makes them particularly relevant for identifying potential outliers or influential points that may affect a model's validity. In l;ogistic regression, where traditional residuals are not meaningful due to the binary nature of the outcome, deviance residuals are an essential diagnostic tool to test a model's assumptions of appropriate fit for individual observations.

@fig-deviance-residuals shows a few observations that fall outside of the range we'd like. Because the Cook's Distance is low, we're not too concerned, but we do want to know if any of the problematic observations here are the same as those identified in @fig-cooks-distance.

List those that cross the threshold of \> 2

```{r}
#| echo: true
#| message: false
#| warning: false

# Calculate deviance residuals from your fitted model
deviance_res <- residuals(model, type = "deviance")

# Combine with observation index for clarity
res_df <- data.frame(
  index = seq_along(deviance_res),
  deviance_residual = deviance_res
)

# Filter for residuals greater than 2
large_residuals <- res_df %>%
  filter(abs(deviance_residual) > 2)

# View top results
head(large_residuals)
```

```{r}
#| label: fig-deviance-residuals
#| fig-cap: "Deviance Residuals"
# Calculate deviance residuals
residuals_dev <- residuals(model, type = "deviance")

# Create a data frame with index and residuals
res_df <- data.frame(index = 1:length(residuals_dev),
                     deviance_residuals = residuals_dev)

# Plot using ggplot2
library(ggplot2)

ggplot(res_df, aes(x = index, y = deviance_residuals)) +
  geom_point(color = "steelblue", alpha = 0.6) +
  geom_hline(yintercept = c(-2,2), linetype = "dashed", color = "red") +
  labs(title = "Deviance Residuals vs Observation Index",
       x = "Observation Index",
       y = "Deviance Residuals") +
  theme_minimal()

```
:::

Considering the model assumptions needs to be done in the context of the study design and your subject matter knowledge of the variables and the data. Here is a list of the assumptions that you should consider (we'll take a closer look at each of them below):

1.  The dependent (outcome) variable is binary
2.  Observations are independent
3.  Adequate sample size
4.  No omitted variables that bias the study
5.  Good logistic regression model fit
6.  No perfect multicollinearity
7.  Linear relationship between the log-odds and predictors
8.  Absence of highly influential outliers

### 1. The Dependent Variable is Binary

-   Assumption: The outcome variable must be binary (eg. "Yes" or "No")

-   Check: Ensure the dependent variable has two levels

    -   In R, use table(your_data\$dependent_variable) to confirm

    -   In our case, the outcome is diabetes or no diabetes

### 2. Observations are Independent

-   Assumption: Each observation is independent of the others.

This means that the outcome of one observation does not influence or is not influenced by the outcome of another.

### 6. Multicollinearity

Multicollinearity can make it difficult to determine the independent effect of each predictor, leading to inflated standard errors, unstable coefficient estimates, and reduced interpretability of the model. While it does not affect the predictive power of the model as a whole, it can compromise the reliability of individual coefficients, making them less useful for inference.

Variance Inflation Factor (VIF) Detects Multivariate Issues:

VIF assesses how much the variance of the regression coefficient is inflated due to linear dependence among predictors. It considers the combined effect of all predictors in the model, not just pairwise correlations.

TO calculate VIF for your logistic regression model, you can use the car package which provides the vif() function. Note that all of the predictor variables included in the model are passed to the vif() function except the interaction terms.

```{r}
#| echo: true
#| message: false
#| warning: false
library(car)

dataset2 <- PimaIndiansDiabetes %>% 
  na.omit()

model <- glm(diabetes ~ glucose + age + pregnant + mass + pressure + pedigree, family = "binomial", data = dataset2)

vif(model)
```

As a rule of thumb, a VIF higher than 10 is an indication of severe multicollinearity. A variable with VIF greater than 10 doesn't always need to be dropped. While a high VIF indicates potential multicollinearity, it is crucial to consider the context, the importance of the variable, and the specific goals of the analysis. There are techniques to address multicollinearity without immediately resorting to dropping variables.
