set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 4
kmodes_model <- kmodes(df %>% select(-converted),
modes = num_clusters,
iter.max = 10,
weighted = FALSE)
install.packages("clustMixType")
library(tidyverse)
library(caret)
library(factoextra)
library(clustMixType)
set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 4
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 4
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 4
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
install.packages("cluster")
library(cluster)
library(tidyverse)
library(caret)
library(factoextra)
library(clustMixType)
library(cluster)
# Load Data
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
# Select relevant columns (drop date & user_id)
df <- df %>% select(-date_served, -user_id)
# Convert all columns to factors (K-Modes works with factors)
df <- df %>% mutate(across(everything(), as.factor))
# Optional: Check for missing values
sum(is.na(df)) # If you have NAs, consider imputation or filtering
set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 4
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
# Load Data
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
# Select relevant columns (drop date & user_id)
df <- df %>% select(-date_served, -user_id)
# Convert all columns to factors (K-Modes works with factors)
df <- df %>% mutate(across(everything(), as.factor))
# Optional: Check for missing values
sum(is.na(df)) # If you have NAs, consider imputation or filtering
set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 2
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
set.seed(123) # Ensure reproducibility
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 2
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
# Choosing number of clusters (start with 3-5 and adjust)
num_clusters <- 2
kmodes_model <- kmeans(daisy(df, metric = "gower"), centers = num_clusters, nstart = 10)
# Print results
print(kmodes_model)
library(tidyverse)
library(caret)
library(randomForest)
# Analyze feature importance (coefficients)
coefficients <- summary(logistic_model)$coefficients
library(tidyverse)
library(caret)
library(randomForest)
# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
# Preprocess data
df <- df %>%
select(-user_id, -date_served)  # Drop unnecessary columns
df$converted <- as.factor(df$converted)  # Ensure the target is a factor
# Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Train logistic regression model
logistic_model <- glm(converted ~ ., data = train_data, family = "binomial")
# Summarize the model
summary(logistic_model)
# Make predictions on the test set
test_data$predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)
# Evaluate model performance
confusion <- confusionMatrix(as.factor(test_data$predicted_class), test_data$converted)
# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
# Step 1: Preprocess data
df <- df %>%
select(-user_id, -date_served)  # Drop unnecessary columns
df$converted <- as.factor(df$converted)  # Ensure the target is a factor
# Step 2: Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 2: Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 3: Train logistic regression model
logistic_model <- glm(converted ~ ., data = train_data, family = "binomial")
# Step 4: Summarize the model
summary(logistic_model)
# Step 5: Make predictions on the test set
test_data$predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)
# Ensure predicted_class and converted are factors with the same levels
test_data$predicted_class <- factor(test_data$predicted_class, levels = c(0, 1))
test_data$converted <- factor(test_data$converted, levels = c(0, 1))
# Step 6: Evaluate model performance
confusion <- confusionMatrix(test_data$predicted_class, test_data$converted)
print(confusion)
# Step 7: Analyze feature importance (coefficients)
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
# Step 8: Plot feature importance
coefficients %>%
filter(Feature != "(Intercept)") %>%
ggplot(aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Feature Importance (Logistic Regression)",
x = "Features", y = "Coefficient Estimate") +
theme_minimal()
# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
# Step 1: Preprocess data
df <- df %>%
select(-user_id, -date_served, -is_retained)  # Drop unnecessary columns
df$converted <- as.factor(df$converted)  # Ensure the target is a factor
# Step 2: Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 3: Train logistic regression model
logistic_model <- glm(converted ~ ., data = train_data, family = "binomial")
# Step 4: Summarize the model
summary(logistic_model)
# Step 5: Make predictions on the test set
test_data$predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)
# Ensure predicted_class and converted are factors with the same levels
test_data$predicted_class <- factor(test_data$predicted_class, levels = c(0, 1))
test_data$converted <- factor(test_data$converted, levels = c(0, 1))
# Step 6: Evaluate model performance
confusion <- confusionMatrix(test_data$predicted_class, test_data$converted)
print(confusion)
# Step 7: Analyze feature importance (coefficients)
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
# Step 8: Plot feature importance
coefficients %>%
filter(Feature != "(Intercept)") %>%
ggplot(aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Feature Importance (Logistic Regression)",
x = "Features", y = "Coefficient Estimate") +
theme_minimal()
# Step 4b: Extract coefficients from the logistic regression summary
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
# Step 2: Filter variables with p-value < 0.05
significant_vars <- coefficients %>%
filter(`Pr(>|z|)` < 0.05 & Feature != "(Intercept)")  # Exclude the intercept
# Step 3: Visualize significant variables
ggplot(significant_vars, aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Statistically Significant Variables (p-value < 0.05)",
x = "Variables",
y = "Coefficient Estimate") +
theme_minimal()
# Step 4a: Summarize the model
summary(logistic_model)
View(df)
print(conversion_rates)
head(df)
# Group data by subscribing_channel and calculate conversion rates
conversion_rates <- df %>%
group_by(subscribing_channel) %>%
summarize(
total_users = n(),
converted_users = sum(as.numeric(converted), na.rm = TRUE),
conversion_rate = (converted_users / total_users) * 100
)
# Debugging: Print grouped data
print(conversion_rates)
# Correct visualization code
ggplot(conversion_rates, aes(x = subscribing_channel, y = conversion_rate)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Conversion Rates by Subscribing Channel",
x = "Subscribing Channel",
y = "Conversion Rate (%)") +
theme_minimal()
# Debugging: Print grouped data
print(converted)
# Debugging: Print grouped data
unique(df$converted)
# Step 8: Plot feature importance
coefficients %>%
filter(Feature != "(Intercept)") %>%
ggplot(aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Feature Importance (Logistic Regression)",
x = "Features", y = "Coefficient Estimate") +
theme_minimal()
# Group data by subscribing_channel and calculate conversion rates
conversion_rates <- df %>%
group_by(subscribing_channel) %>%
summarize(
total_users = n(),  # Total users per channel
converted_users = sum(converted, na.rm = TRUE),  # Count of TRUE values in 'converted'
conversion_rate = (converted_users / total_users) * 100  # Percentage of users converted
)
# Group data by subscribing_channel and calculate conversion rates
df$converted <- as.numeric(as.logical(df$converted))
conversion_rates <- df %>%
group_by(subscribing_channel) %>%
summarize(
total_users = n(),  # Total users per channel
converted_users = sum(converted, na.rm = TRUE),  # Count of TRUE values in 'converted'
conversion_rate = (converted_users / total_users) * 100  # Percentage of users converted
)
# Print the calculated conversion rates
print(conversion_rates)
# Visualize conversion rates
ggplot(conversion_rates, aes(x = subscribing_channel, y = conversion_rate)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Conversion Rates by Subscribing Channel",
x = "Subscribing Channel",
y = "Conversion Rate (%)") +
theme_minimal()
library(randomForest)
rf_model <- randomForest(as.factor(converted) ~ ., data = df, importance = TRUE)
importance(rf_model)
varImpPlot(rf_model)
# Step 4b: Extract coefficients from the logistic regression summary
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
# Step 2: Filter variables with p-value < 0.05
significant_vars <- coefficients %>%
filter(`Pr(>|z|)` < 0.05 & Feature != "(Intercept)")  # Exclude the intercept
# Step 3: Visualize significant variables
ggplot(significant_vars, aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Statistically Significant Variables (p-value < 0.05)",
x = "Variables",
y = "Coefficient Estimate") +
theme_minimal()
# Step 5: Make predictions on the test set
test_data$predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)
# Ensure predicted_class and converted are factors with the same levels
test_data$predicted_class <- factor(test_data$predicted_class, levels = c(0, 1))
test_data$converted <- factor(test_data$converted, levels = c(0, 1))
# Step 6: Evaluate model performance
confusion <- confusionMatrix(test_data$predicted_class, test_data$converted)
print(confusion)
# Step 7: Analyze feature importance (coefficients)
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
library(tidyverse)
library(caret)
library(randomForest)
# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
# Step 1: Preprocess data
df <- df %>%
select(-user_id, -date_served, -is_retained)  # Drop unnecessary columns
df$converted <- as.factor(df$converted)  # Ensure the target is a factor
# Step 2: Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 2: Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
# Step 3: Train logistic regression model
logistic_model <- glm(converted ~ ., data = train_data, family = "binomial")
# Step 4a: Summarize the model
summary(logistic_model)
# Step 5: Make predictions on the test set
test_data$predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)
# Ensure predicted_class and converted are factors with the same levels
test_data$predicted_class <- factor(test_data$predicted_class, levels = c(0, 1))
test_data$converted <- factor(test_data$converted, levels = c(0, 1))
# Step 6: Evaluate model performance
confusion <- confusionMatrix(test_data$predicted_class, test_data$converted)
print(confusion)
# Step 7: Analyze feature importance (coefficients)
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
# Step 8: Plot feature importance
coefficients %>%
filter(Feature != "(Intercept)") %>%
ggplot(aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Feature Importance (Logistic Regression)",
x = "Features", y = "Coefficient Estimate") +
theme_minimal()
# Step 4b: Extract coefficients from the logistic regression summary
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
# Step 2: Filter variables with p-value < 0.05
significant_vars <- coefficients %>%
filter(`Pr(>|z|)` < 0.05 & Feature != "(Intercept)")  # Exclude the intercept
# Step 3: Visualize significant variables
ggplot(significant_vars, aes(x = reorder(Feature, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +
labs(title = "Statistically Significant Variables (p-value < 0.05)",
x = "Variables",
y = "Coefficient Estimate") +
theme_minimal()
# Group data by subscribing_channel and calculate conversion rates
df$converted <- as.numeric(as.logical(df$converted))
conversion_rates <- df %>%
group_by(subscribing_channel) %>%
summarize(
total_users = n(),  # Total users per channel
converted_users = sum(converted, na.rm = TRUE),  # Count of TRUE values in 'converted'
conversion_rate = (converted_users / total_users) * 100  # Percentage of users converted
)
# Print the calculated conversion rates
print(conversion_rates)
# Visualize conversion rates
ggplot(conversion_rates, aes(x = subscribing_channel, y = conversion_rate)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Conversion Rates by Subscribing Channel",
x = "Subscribing Channel",
y = "Conversion Rate (%)") +
theme_minimal()
# Group data by subscribing_channel and calculate conversion rates
df$converted <- as.numeric(as.logical(df$converted))
conversion_rates <- df %>%
group_by(subscribing_channel) %>%
summarize(
total_users = n(),  # Total users per channel
converted_users = sum(converted, na.rm = TRUE),  # Count of TRUE values in 'converted'
conversion_rate = (converted_users / total_users) * 100  # Percentage of users converted
)
# Print the calculated conversion rates
# print(conversion_rates)
# Visualize conversion rates
ggplot(conversion_rates, aes(x = subscribing_channel, y = conversion_rate)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Conversion Rates by Subscribing Channel",
x = "Subscribing Channel",
y = "Conversion Rate (%)") +
theme_minimal()
library(randomForest)
rf_model <- randomForest(as.factor(converted) ~ ., data = df, importance = TRUE)
importance(rf_model)
varImpPlot(rf_model)
df %>%
table(subscribing_channel, converted)
df %>%
print(table(subscribing_channel, converted))
df %>%
group_by(subscribing_channel) %>%
summarise(
total = n(),
converted = sum(converted == "TRUE"),
percent_converted = (converted / total) * 100
) %>%
arrange(desc(percent_converted))
# Ensure 'converted' is logical
df <- df %>%
mutate(converted = as.logical(converted))
# Group by subscribing channel and calculate conversion rate
df %>%
group_by(subscribing_channel) %>%
summarise(
total = n(),
converted = sum(converted, na.rm = TRUE),
percent_converted = (converted / total) * 100
) %>%
arrange(desc(percent_converted))
# Ensure 'converted' is logical
df <- df %>%
mutate(converted = as.logical(converted))
# Group by subscribing channel and calculate conversion rate
df %>%
group_by(subscribing_channel) %>%
summarise(
total = n(),
converted = sum(converted, na.rm = TRUE),
percent_converted = round((converted / total) * 100, 2)
) %>%
arrange(desc(percent_converted))
# Ensure 'converted' is logical
df <- df %>%
mutate(converted = as.logical(converted))
# Group by subscribing channel and calculate conversion rate
df %>%
group_by(subscribing_channel) %>%
summarise(
total = n(),
converted = sum(converted, na.rm = TRUE),
percent_converted = round((converted / total) * 100, 2)
) %>%
arrange(desc(percent_converted))
# Group data by subscribing_channel and calculate conversion rates
df$converted <- as.numeric(as.logical(df$converted))
conversion_rates <- df %>%
group_by(subscribing_channel) %>%
summarize(
total_users = n(),  # Total users per channel
converted_users = sum(converted, na.rm = TRUE),  # Count of TRUE values in 'converted'
conversion_rate = (converted_users / total_users) * 100  # Percentage of users converted
)
# Print the calculated conversion rates
# print(conversion_rates)
# Visualize conversion rates
ggplot(conversion_rates, aes(x = subscribing_channel, y = conversion_rate)) +
geom_bar(stat = "identity", fill = "steelblue") +
labs(title = "Conversion Rates by Subscribing Channel",
x = "Subscribing Channel",
y = "Conversion Rate (%)") +
theme_minimal()
df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
install.packages("rpart")
df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
library(rpart)
tree_model <- rpart(is_retained ~ ., data = df, method = "class")
library(rpart)
tree_model <- rpart(is_retained ~ ., data = df, method = "class")
df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
View(df)
# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
glm_model <- glm(is_retained ~ ., data = df, family = "binomial")  # Logistic regression
summary(glm_model)  # View significant predictors
library(tidyverse)
library(caret)
library(randomForest)
# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
glm_model <- glm(is_retained ~ ., data = df, family = "binomial")  # Logistic regression
summary(glm_model)  # View significant predictors
# Preprocess data
df <- df %>%
select(-user_id, -date_served)  # Drop unnecessary columns
df$converted <- as.factor(df$converted)  # Ensure the target is a factor
df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
glm_model <- glm(is_retained ~ ., data = df, family = "binomial")  # Logistic regression
summary(glm_model)  # View significant predictors
df$is_retained <- as.factor(df$is_retained)
# Load Libraries
library(rpart)
# Prepare the data
df$is_retained <- as.factor(df$is_retained)
# Build decision tree model
tree_model <- rpart(is_retained ~ subscribing_channel + dietary_preference + meal_type + location + age_group,
data = df, method = "class",
control = rpart.control(cp = 0.01))  # Complexity parameter to avoid overfitting
# Visualize the tree
rpart.plot(tree_model, type = 3, extra = 104, fallen.leaves = TRUE)
library(rpart.plot)
install.packages("rpart.plot")
# Load Libraries
library(rpart)
library(rpart.plot)
# Prepare the data
df$is_retained <- as.factor(df$is_retained)
# Build decision tree model
tree_model <- rpart(is_retained ~ subscribing_channel + dietary_preference + meal_type + location + age_group,
data = df, method = "class",
control = rpart.control(cp = 0.01))  # Complexity parameter to avoid overfitting
# Visualize the tree
rpart.plot(tree_model, type = 3, extra = 104, fallen.leaves = TRUE)
# Evaluate performance
preds <- predict(tree_model, df, type = "class")
conf_matrix <- table(Predicted = preds, Actual = df$is_retained)
print(conf_matrix)
install.packages("ROSE")
library(ROSE)
df_balanced <- ROSE(is_retained ~ ., data = df, seed = 123)$data
