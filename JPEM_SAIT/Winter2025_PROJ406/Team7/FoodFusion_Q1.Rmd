---
title: "FoodFusion Q1: What drives user conversion?"
author: "Jason Pemberton"
date: "2025-03-21"
output: html_document
---

1.  What drives user conversion?

Business insight: Identify the key factors influencing whether a user converts.\
**Approach:**

-   Use Logistic Regression or Random Forest Classifier to model the relationship between the predictors ***subscribing***, ***dietary_pre***, ***meal_type***, ***location***, ***age_group***) and the target variable ***converted***

-   Evaluate feature importance to understand the weight of each factor on conversions.

-   Provide actionable insights, such as targeting specific user segments for better conversion rates.

Output: A ranked list of factors that drive conversions

---

STEP 1: Load Libraries

```{r Load Libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(randomForest)
```

STEP 2: Load Data

```{r Load Data, echo=TRUE, message=FALSE, warning=FALSE}

# Load dataset
df <- read.csv("C:/Users/jason/OneDrive/SAIT/SAIT Instructor/2025-2026/WINTER 2025 - COURSE - PROJ406/Team 7/data/foodfusion.csv")
```

STEP 3: Data Preparation

```{r Select Necessary Columns and Prepare for Model, echo=TRUE, message=FALSE, warning=FALSE}
# Preprocess data
df <- df %>%
  select(-user_id, -date_served)  # Drop unnecessary columns
df$converted <- as.factor(df$converted)  # Ensure the target is a factor
```

STEP 4: Data Partition

```{r Partition Data, echo=TRUE, message=FALSE, warning=FALSE}
# Split data into training and testing sets
set.seed(123)  # For reproducibility
train_index <- createDataPartition(df$converted, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
```

STEP 5: Train GLM Model

```{r Train GLM Model, echo=TRUE, message=FALSE, warning=FALSE}
# Train logistic regression model
logistic_model <- glm(converted ~ ., data = train_data, family = "binomial")
```

STEP 6: Summarize Model Results

```{r Summarize Model Results, echo=FALSE, message=FALSE, warning=FALSE}
# Summarize the model
summary(logistic_model)
```

STEP 7: Make Predictions on the Test data

```{r Model Predictions, echo=TRUE, message=FALSE, warning=FALSE}
# Make predictions on the test set
test_data$predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)

# Ensure predicted_class and converted are factors with the same levels
test_data$predicted_class <- factor(test_data$predicted_class, levels = c(0, 1))
test_data$converted <- factor(test_data$converted, levels = c(0, 1))
```

STEP 8: Evaluate Model Performance

```{r Evaluate Model, echo=FALSE, message=FALSE, warning=FALSE}
# Evaluate model performance
confusion <- confusionMatrix(test_data$predicted_class, test_data$converted)
print(confusion)
```

STEP 9: Feature Importance

```{r Feature Importance 1, echo=FALSE, message=FALSE, warning=FALSE}
# Analyze feature importance (coefficients)
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL
```

STEP 10: Visualize Feature Importance

```{r Feature Importance 2, echo=FALSE, message=FALSE, warning=FALSE}
# Plot feature importance
coefficients %>%
  filter(Feature != "(Intercept)") %>%
  ggplot(aes(x = reorder(Feature, Estimate), y = Estimate)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Feature Importance (Logistic Regression)",
       x = "Features", y = "Coefficient Estimate") +
  theme_minimal()
```

STEP 11: Top Features

```{r Visualize Most Significant Variables, echo=FALSE, message=FALSE, warning=FALSE}
# Extract coefficients from the logistic regression summary
coefficients <- summary(logistic_model)$coefficients
coefficients <- as.data.frame(coefficients)
coefficients$Feature <- rownames(coefficients)
rownames(coefficients) <- NULL

# Step 2: Filter variables with p-value < 0.05
significant_vars <- coefficients %>%
  filter(`Pr(>|z|)` < 0.05 & Feature != "(Intercept)")  # Exclude the intercept

# Step 3: Visualize significant variables

ggplot(significant_vars, aes(x = reorder(Feature, Estimate), y = Estimate)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Statistically Significant Variables (p-value < 0.05)",
       x = "Variables",
       y = "Coefficient Estimate") +
  theme_minimal()
```

STEP 12: Conversion Rates by Subscribing Channel

```{r Conversion Rates, echo=FALSE, message=FALSE, warning=FALSE}
# Ensure 'converted' is logical
df <- df %>%
  mutate(converted = as.logical(converted))

# Group by subscribing channel and calculate conversion rate
df %>%
  group_by(subscribing_channel) %>%
  summarise(
    total = n(),
    converted = sum(converted, na.rm = TRUE),
    percent_converted = round((converted / total) * 100, 2)
  ) %>%
  arrange(desc(percent_converted))



# Group data by subscribing_channel and calculate conversion rates
df$converted <- as.numeric(as.logical(df$converted))

conversion_rates <- df %>%
  group_by(subscribing_channel) %>%
  summarize(
    total_users = n(),  # Total users per channel
    converted_users = sum(converted, na.rm = TRUE),  # Count of TRUE values in 'converted'
    conversion_rate = (converted_users / total_users) * 100  # Percentage of users converted
  )

# Print the calculated conversion rates
# print(conversion_rates)

# Visualize conversion rates
ggplot(conversion_rates, aes(x = subscribing_channel, y = conversion_rate)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Conversion Rates by Subscribing Channel",
       x = "Subscribing Channel",
       y = "Conversion Rate (%)") +
  theme_minimal()
```

STEP 13: Random Forest Model

```{r Random Forest Model, echo=FALSE, message=FALSE, warning=FALSE}
rf_model <- randomForest(as.factor(converted) ~ ., data = df, importance = TRUE)
importance(rf_model)
varImpPlot(rf_model)
```

---

**ADDITIONAL**

#### Logistic Regression (GLM):

-   Works well with **encoded categorical data**.

-    Provides **coefficients** to explain which features (e.g., subscribing channel) are most important.

<!-- -->

-    Output: Probability of **"is_retained" = TRUE** for each customer.

```{r Simple GLM}

df$is_retained <- as.factor(df$is_retained)  # Ensure target is categorical
glm_model <- glm(is_retained ~ ., data = df, family = "binomial")  # Logistic regression
summary(glm_model)  # View significant predictors
```

---

**Interpretation of Significant Variables**

**Subscribing Channel (House Ads)**

-    **Estimate: 0.2153, p = 0.0289** (*Significant, p \< 0.05*)

-    Interpretation: Users who subscribed via House Ads are significantly more likely to be retained compared to the baseline subscribing channel.

**Dietary Preference (Keto)**

-    **Estimate: -0.2107, p = 0.0178** (*Significant, p \< 0.05*

-    Interpretation: Keto users are significantly *less* likely to be retained compared to the baseline dietary preference.

**Dietary Preference (Vegan)**

-    **Estimate: -0.1748, p = 0.0510** (*Marginally Significant, p \~ 0.05*)

-    Interpretation: Vegan users might also be less likely to be retained, but it's borderline.

---

#### **Recommendations for FoodFusion**

1.   **Optimize House Ads** → Since it significantly increases retention, FoodFusion should invest more in this channel.

2.   **Investigate Keto Dieters** → They seem less likely to be retained. Maybe they need different marketing, better meal options, or a different onboarding approach.

3.   **Ignore Insignificant Variables for Now** → Since age, location, and other meal types don’t have a strong effect, there’s no need to focus marketing efforts there.

---

#### Decision Tree:

-    Works **without encoding** categorical data.

-    Outputs a tree structure showing how different factors affect retention.

```{r}
# Load Libraries
library(rpart)
library(rpart.plot)
library(ROSE)

# Prepare the data
df$is_retained <- as.factor(df$is_retained)
df_balanced <- ROSE(is_retained ~ ., data = df, seed = 123)$data

# Build decision tree model
tree_model <- rpart(is_retained ~ subscribing_channel + dietary_preference + meal_type + location + age_group, 
                     data = df, method = "class", 
                     control = rpart.control(cp = 0.01))  # Complexity parameter to avoid overfitting

# Visualize the tree
rpart.plot(tree_model, type = 3, extra = 104, fallen.leaves = TRUE)

# Evaluate performance
preds <- predict(tree_model, df, type = "class")
conf_matrix <- table(Predicted = preds, Actual = df$is_retained)
print(conf_matrix)

```

#### Random Forest:

-   Better Predictions, Less Overfitting

-    Works well for categorical data.

-    Can handle **complex relationships**.

-    Outputs **feature importance** to rank the most important factors for retention.

```{r}
df$is_retained <- as.factor(df$is_retained)  # Convert target to factor
rf_model <- randomForest(is_retained ~ ., data = df, ntree = 100)
print(rf_model)
importance(rf_model)  # View most important features
```
